{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"5\"><b>Final Project Report Group 42- Heart Failure Fatality</b></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Introduction**\n",
    "    \n",
    "Cardiovascular diseases kill approximately 17 million people in the world, including heart attacks, strokes, and heart failure. In particular, heart failure is caused when the heart cannot successfully send the required amount of blood to the body (Chicco, 2020), and can occur for numerous reasons such as diabetes, high blood pressure, and other heart conditions or diseases. With the emergence and accessibility of electronic health records, it is now possible to use data from patients who have experienced heart failure and find trends and patterns amongst variables that could be possible predictors for people who are at risk of heart failure. Due to the vital nature of the heart, finding trends among variables as predictors of heart failure has become a priority among doctors and researchers alike. Our project was to take a data set comprised of data taken from people who had suffered from heart failure and build a classifying model that could predict a person's chance of survival after heart failure. A data set collected at the Faisalabad Institute of Cardiology and at the Allied Hospital in Faisalabad contains the data of 299 patients which consisted of 105 women and 194 men who had all experienced heart failure. The dataset contained 13 features which ranged from physical, clinical, and lifestyle characteristics. This included categorical variables like sex, diabetes, and smoking; as well as numerical variables such as platelet count, serum creatine levels, and ejection fraction. This data set also contained the information on if the patient had died within 130 days of their heart failure, referred to as a death event. Through the use of forward selection, we were able to identify two variables that were strong predictors of death events. These two variables, serum creatine and ejection fraction were used to build our classifying model that would take in the values of our predictors and classify whether this patient would experience a death event or not following 130 days after heart failure. This classifier would allow us to answer the question of, if given a person's serum creatine levels and ejection fraction will that person survive within 130 days after their heart failure?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Methods and Results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "── \u001b[1mAttaching packages\u001b[22m ─────────────────────────────────────── tidyverse 1.3.0 ──\n",
      "\n",
      "\u001b[32m✔\u001b[39m \u001b[34mggplot2\u001b[39m 3.3.2     \u001b[32m✔\u001b[39m \u001b[34mpurrr  \u001b[39m 0.3.4\n",
      "\u001b[32m✔\u001b[39m \u001b[34mtibble \u001b[39m 3.0.3     \u001b[32m✔\u001b[39m \u001b[34mdplyr  \u001b[39m 1.0.2\n",
      "\u001b[32m✔\u001b[39m \u001b[34mtidyr  \u001b[39m 1.1.2     \u001b[32m✔\u001b[39m \u001b[34mstringr\u001b[39m 1.4.0\n",
      "\u001b[32m✔\u001b[39m \u001b[34mreadr  \u001b[39m 1.3.1     \u001b[32m✔\u001b[39m \u001b[34mforcats\u001b[39m 0.5.0\n",
      "\n",
      "Warning message:\n",
      "“package ‘ggplot2’ was built under R version 4.0.1”\n",
      "Warning message:\n",
      "“package ‘tibble’ was built under R version 4.0.2”\n",
      "Warning message:\n",
      "“package ‘tidyr’ was built under R version 4.0.2”\n",
      "Warning message:\n",
      "“package ‘dplyr’ was built under R version 4.0.2”\n",
      "── \u001b[1mConflicts\u001b[22m ────────────────────────────────────────── tidyverse_conflicts() ──\n",
      "\u001b[31m✖\u001b[39m \u001b[34mdplyr\u001b[39m::\u001b[32mfilter()\u001b[39m masks \u001b[34mstats\u001b[39m::filter()\n",
      "\u001b[31m✖\u001b[39m \u001b[34mdplyr\u001b[39m::\u001b[32mlag()\u001b[39m    masks \u001b[34mstats\u001b[39m::lag()\n",
      "\n",
      "Warning message:\n",
      "“package ‘tidymodels’ was built under R version 4.0.2”\n",
      "── \u001b[1mAttaching packages\u001b[22m ────────────────────────────────────── tidymodels 0.1.1 ──\n",
      "\n",
      "\u001b[32m✔\u001b[39m \u001b[34mbroom    \u001b[39m 0.7.0      \u001b[32m✔\u001b[39m \u001b[34mrecipes  \u001b[39m 0.1.13\n",
      "\u001b[32m✔\u001b[39m \u001b[34mdials    \u001b[39m 0.0.9      \u001b[32m✔\u001b[39m \u001b[34mrsample  \u001b[39m 0.0.7 \n",
      "\u001b[32m✔\u001b[39m \u001b[34minfer    \u001b[39m 0.5.4      \u001b[32m✔\u001b[39m \u001b[34mtune     \u001b[39m 0.1.1 \n",
      "\u001b[32m✔\u001b[39m \u001b[34mmodeldata\u001b[39m 0.0.2      \u001b[32m✔\u001b[39m \u001b[34mworkflows\u001b[39m 0.2.0 \n",
      "\u001b[32m✔\u001b[39m \u001b[34mparsnip  \u001b[39m 0.1.3      \u001b[32m✔\u001b[39m \u001b[34myardstick\u001b[39m 0.0.7 \n",
      "\n",
      "Warning message:\n",
      "“package ‘broom’ was built under R version 4.0.2”\n",
      "Warning message:\n",
      "“package ‘dials’ was built under R version 4.0.2”\n",
      "Warning message:\n",
      "“package ‘infer’ was built under R version 4.0.3”\n",
      "Warning message:\n",
      "“package ‘modeldata’ was built under R version 4.0.1”\n",
      "Warning message:\n",
      "“package ‘parsnip’ was built under R version 4.0.2”\n",
      "Warning message:\n",
      "“package ‘recipes’ was built under R version 4.0.1”\n",
      "Warning message:\n",
      "“package ‘tune’ was built under R version 4.0.2”\n",
      "Warning message:\n",
      "“package ‘workflows’ was built under R version 4.0.2”\n",
      "Warning message:\n",
      "“package ‘yardstick’ was built under R version 4.0.2”\n",
      "── \u001b[1mConflicts\u001b[22m ───────────────────────────────────────── tidymodels_conflicts() ──\n",
      "\u001b[31m✖\u001b[39m \u001b[34mscales\u001b[39m::\u001b[32mdiscard()\u001b[39m masks \u001b[34mpurrr\u001b[39m::discard()\n",
      "\u001b[31m✖\u001b[39m \u001b[34mdplyr\u001b[39m::\u001b[32mfilter()\u001b[39m   masks \u001b[34mstats\u001b[39m::filter()\n",
      "\u001b[31m✖\u001b[39m \u001b[34mrecipes\u001b[39m::\u001b[32mfixed()\u001b[39m  masks \u001b[34mstringr\u001b[39m::fixed()\n",
      "\u001b[31m✖\u001b[39m \u001b[34mdplyr\u001b[39m::\u001b[32mlag()\u001b[39m      masks \u001b[34mstats\u001b[39m::lag()\n",
      "\u001b[31m✖\u001b[39m \u001b[34myardstick\u001b[39m::\u001b[32mspec()\u001b[39m masks \u001b[34mreadr\u001b[39m::spec()\n",
      "\u001b[31m✖\u001b[39m \u001b[34mrecipes\u001b[39m::\u001b[32mstep()\u001b[39m   masks \u001b[34mstats\u001b[39m::step()\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Loading necessary packages\n",
    "library(tidyverse)\n",
    "library(tidymodels)\n",
    "library(repr)\n",
    "\n",
    "# Setting the seed to ensure reproducability \n",
    "set.seed(42)\n",
    "\n",
    "# Setting the number of maximum rows which will be displayed whenever a data tibble needs to be outputed \n",
    "options(repr.matrix.max.rows = 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, appropriate packages were added to the file to use their inbuilt functions. A seed was set to ensure reproducibility so that the random values are always the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsed with column specification:\n",
      "cols(\n",
      "  age = \u001b[32mcol_double()\u001b[39m,\n",
      "  anaemia = \u001b[32mcol_double()\u001b[39m,\n",
      "  creatinine_phosphokinase = \u001b[32mcol_double()\u001b[39m,\n",
      "  diabetes = \u001b[32mcol_double()\u001b[39m,\n",
      "  ejection_fraction = \u001b[32mcol_double()\u001b[39m,\n",
      "  high_blood_pressure = \u001b[32mcol_double()\u001b[39m,\n",
      "  platelets = \u001b[32mcol_double()\u001b[39m,\n",
      "  serum_creatinine = \u001b[32mcol_double()\u001b[39m,\n",
      "  serum_sodium = \u001b[32mcol_double()\u001b[39m,\n",
      "  sex = \u001b[32mcol_double()\u001b[39m,\n",
      "  smoking = \u001b[32mcol_double()\u001b[39m,\n",
      "  time = \u001b[32mcol_double()\u001b[39m,\n",
      "  DEATH_EVENT = \u001b[32mcol_double()\u001b[39m\n",
      ")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#reads in data as well as turns relevant double columns into factors \n",
    "url <- \"https://archive.ics.uci.edu/ml/machine-learning-databases/00519/heart_failure_clinical_records_dataset.csv\"\n",
    "\n",
    "# tranforms the data variables which need to factors into the factor datatype \n",
    "heart_data <- read_csv(url)%>%\n",
    "    mutate(sex = as_factor(sex))%>%\n",
    "    mutate(smoking = as_factor(smoking))%>%\n",
    "    mutate(DEATH_EVENT = as_factor(DEATH_EVENT))%>%\n",
    "    mutate(high_blood_pressure = as_factor(high_blood_pressure))%>%\n",
    "    mutate(diabetes = as_factor(diabetes))%>%\n",
    "    mutate(anaemia= as_factor(anaemia)) \n",
    "\n",
    "\n",
    "#creates factor levels for columns that make sense with column name, not just 0 and 1 \n",
    "levels(heart_data$sex) <- c(\"female\",\"male\")\n",
    "levels(heart_data$smoking) <- c(\"no\",\"yes\")\n",
    "levels(heart_data$DEATH_EVENT) <- c(\"died\",\"survived\")\n",
    "levels(heart_data$high_blood_pressure) <- c(\"no\",\"yes\")\n",
    "levels(heart_data$diabetes) <- c(\"no\",\"yes\")\n",
    "levels(heart_data$anaemia) <- c(\"no\",\"yes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, the dataset was read as csv file. All the catagorical variables were transformed into factors datatype by using mutate function and as_factor function. Then, their factor levels were set to understandable values rather than just 0 and 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<caption>A tibble: 225 × 13</caption>\n",
       "<thead>\n",
       "\t<tr><th scope=col>age</th><th scope=col>anaemia</th><th scope=col>creatinine_phosphokinase</th><th scope=col>diabetes</th><th scope=col>ejection_fraction</th><th scope=col>high_blood_pressure</th><th scope=col>platelets</th><th scope=col>serum_creatinine</th><th scope=col>serum_sodium</th><th scope=col>sex</th><th scope=col>smoking</th><th scope=col>time</th><th scope=col>DEATH_EVENT</th></tr>\n",
       "\t<tr><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;fct&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;fct&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;fct&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;fct&gt;</th><th scope=col>&lt;fct&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;fct&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><td>75</td><td>no </td><td>582</td><td>no</td><td>20</td><td>yes</td><td>265000</td><td>1.9</td><td>130</td><td>male</td><td>no </td><td>4</td><td>survived</td></tr>\n",
       "\t<tr><td>50</td><td>yes</td><td>111</td><td>no</td><td>20</td><td>no </td><td>210000</td><td>1.9</td><td>137</td><td>male</td><td>no </td><td>7</td><td>survived</td></tr>\n",
       "\t<tr><td>90</td><td>yes</td><td> 47</td><td>no</td><td>40</td><td>yes</td><td>204000</td><td>2.1</td><td>132</td><td>male</td><td>yes</td><td>8</td><td>survived</td></tr>\n",
       "\t<tr><td>⋮</td><td>⋮</td><td>⋮</td><td>⋮</td><td>⋮</td><td>⋮</td><td>⋮</td><td>⋮</td><td>⋮</td><td>⋮</td><td>⋮</td><td>⋮</td><td>⋮</td></tr>\n",
       "\t<tr><td>55</td><td>no</td><td>1820</td><td>no </td><td>38</td><td>no</td><td>270000</td><td>1.2</td><td>139</td><td>female</td><td>no </td><td>271</td><td>died</td></tr>\n",
       "\t<tr><td>45</td><td>no</td><td>2060</td><td>yes</td><td>60</td><td>no</td><td>742000</td><td>0.8</td><td>138</td><td>female</td><td>no </td><td>278</td><td>died</td></tr>\n",
       "\t<tr><td>45</td><td>no</td><td>2413</td><td>no </td><td>38</td><td>no</td><td>140000</td><td>1.4</td><td>140</td><td>male  </td><td>yes</td><td>280</td><td>died</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A tibble: 225 × 13\n",
       "\\begin{tabular}{lllllllllllll}\n",
       " age & anaemia & creatinine\\_phosphokinase & diabetes & ejection\\_fraction & high\\_blood\\_pressure & platelets & serum\\_creatinine & serum\\_sodium & sex & smoking & time & DEATH\\_EVENT\\\\\n",
       " <dbl> & <fct> & <dbl> & <fct> & <dbl> & <fct> & <dbl> & <dbl> & <dbl> & <fct> & <fct> & <dbl> & <fct>\\\\\n",
       "\\hline\n",
       "\t 75 & no  & 582 & no & 20 & yes & 265000 & 1.9 & 130 & male & no  & 4 & survived\\\\\n",
       "\t 50 & yes & 111 & no & 20 & no  & 210000 & 1.9 & 137 & male & no  & 7 & survived\\\\\n",
       "\t 90 & yes &  47 & no & 40 & yes & 204000 & 2.1 & 132 & male & yes & 8 & survived\\\\\n",
       "\t ⋮ & ⋮ & ⋮ & ⋮ & ⋮ & ⋮ & ⋮ & ⋮ & ⋮ & ⋮ & ⋮ & ⋮ & ⋮\\\\\n",
       "\t 55 & no & 1820 & no  & 38 & no & 270000 & 1.2 & 139 & female & no  & 271 & died\\\\\n",
       "\t 45 & no & 2060 & yes & 60 & no & 742000 & 0.8 & 138 & female & no  & 278 & died\\\\\n",
       "\t 45 & no & 2413 & no  & 38 & no & 140000 & 1.4 & 140 & male   & yes & 280 & died\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A tibble: 225 × 13\n",
       "\n",
       "| age &lt;dbl&gt; | anaemia &lt;fct&gt; | creatinine_phosphokinase &lt;dbl&gt; | diabetes &lt;fct&gt; | ejection_fraction &lt;dbl&gt; | high_blood_pressure &lt;fct&gt; | platelets &lt;dbl&gt; | serum_creatinine &lt;dbl&gt; | serum_sodium &lt;dbl&gt; | sex &lt;fct&gt; | smoking &lt;fct&gt; | time &lt;dbl&gt; | DEATH_EVENT &lt;fct&gt; |\n",
       "|---|---|---|---|---|---|---|---|---|---|---|---|---|\n",
       "| 75 | no  | 582 | no | 20 | yes | 265000 | 1.9 | 130 | male | no  | 4 | survived |\n",
       "| 50 | yes | 111 | no | 20 | no  | 210000 | 1.9 | 137 | male | no  | 7 | survived |\n",
       "| 90 | yes |  47 | no | 40 | yes | 204000 | 2.1 | 132 | male | yes | 8 | survived |\n",
       "| ⋮ | ⋮ | ⋮ | ⋮ | ⋮ | ⋮ | ⋮ | ⋮ | ⋮ | ⋮ | ⋮ | ⋮ | ⋮ |\n",
       "| 55 | no | 1820 | no  | 38 | no | 270000 | 1.2 | 139 | female | no  | 271 | died |\n",
       "| 45 | no | 2060 | yes | 60 | no | 742000 | 0.8 | 138 | female | no  | 278 | died |\n",
       "| 45 | no | 2413 | no  | 38 | no | 140000 | 1.4 | 140 | male   | yes | 280 | died |\n",
       "\n"
      ],
      "text/plain": [
       "    age anaemia creatinine_phosphokinase diabetes ejection_fraction\n",
       "1   75  no      582                      no       20               \n",
       "2   50  yes     111                      no       20               \n",
       "3   90  yes      47                      no       40               \n",
       "⋮   ⋮   ⋮       ⋮                        ⋮        ⋮                \n",
       "223 55  no      1820                     no       38               \n",
       "224 45  no      2060                     yes      60               \n",
       "225 45  no      2413                     no       38               \n",
       "    high_blood_pressure platelets serum_creatinine serum_sodium sex    smoking\n",
       "1   yes                 265000    1.9              130          male   no     \n",
       "2   no                  210000    1.9              137          male   no     \n",
       "3   yes                 204000    2.1              132          male   yes    \n",
       "⋮   ⋮                   ⋮         ⋮                ⋮            ⋮      ⋮      \n",
       "223 no                  270000    1.2              139          female no     \n",
       "224 no                  742000    0.8              138          female no     \n",
       "225 no                  140000    1.4              140          male   yes    \n",
       "    time DEATH_EVENT\n",
       "1   4    survived   \n",
       "2   7    survived   \n",
       "3   8    survived   \n",
       "⋮   ⋮    ⋮          \n",
       "223 271  died       \n",
       "224 278  died       \n",
       "225 280  died       "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#splits the data into training and testing data \n",
    "heart_data_split <- initial_split(heart_data, prop = .75, strata = DEATH_EVENT)\n",
    "heart_train <- training(heart_data_split)\n",
    "heart_test <- testing(heart_data_split)\n",
    " \n",
    "heart_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table 1: Table containing training data that our model will be trained with"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the dataset was split into training data and testing data for classification. This function shuffles the data so no particular order/pattern is present in the 2 sets, and also stratifies the data so both sets have the same proportion of positive and negative classes. This makes the 2 sets comparable.\n",
    "\n",
    "The proportion 75% was chosen to use the vast majority of data for training while keeping sufficient number of observations for testing, regarding that our dataset only has 299 observations. This helps our model to be trained on the majority of the data hence being more accurate, while giving us sufficient data to evaluate its accuracy as well. This is one of the splits suggested in the course book as well (refer to the end of the notebook for reference). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove non essential columns from the dataset and only keep numerical to run forward propagation\n",
    "heart_train <- heart_train %>% \n",
    "    select(-anaemia, -diabetes, -high_blood_pressure, -sex, -smoking, -time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, due to medical significance (Chicco, 2020), we decided that we will use \"DEATH_EVENT\" as our output variable which we will try to classify, hence we removed all the other categorical datatypes from the heart_test dataset using the select function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE TO SELECT THE BEST PREDICTOR VARIABLES (code has been adapted from the DSCI 100 course book.) \n",
    "# (Citation is in references section)\n",
    "\n",
    "# Evaluating which predictive variables to choose\n",
    "\n",
    "# creating an object of names of all predictive variables called \"names\"\n",
    "names <- colnames(heart_train %>% select(-DEATH_EVENT))\n",
    "\n",
    "# creating an empty tibble to store the final results\n",
    "accuracies <- tibble(size = integer(), \n",
    "                     model_string = character(), # this is the first argument to the \"recipe\" function\n",
    "                     accuracy = numeric())\n",
    "\n",
    "# creating a model specification for the classifier\n",
    "heart_spec <- nearest_neighbor(weight_func = \"rectangular\", neighbors = tune()) %>% \n",
    "     set_engine(\"kknn\") %>% \n",
    "     set_mode(\"classification\")\n",
    "\n",
    "# creating a dataframe of all candidate k values \n",
    "k_vals <- tibble(neighbors = seq(1:15))\n",
    "\n",
    "# creating a 5-fold cross-validation object\n",
    "heart_vfold <- vfold_cv(heart_train, v = 5, strata = DEATH_EVENT)\n",
    "\n",
    "# storing the total number of predictors in object \"n_total\"\n",
    "n_total <- length(names)\n",
    "\n",
    "# stores the selected predictors\n",
    "selected <- c()\n",
    "\n",
    "# for every size from 1 to the total number of predictors\n",
    "for (i in 1:n_total) {\n",
    "    # for every predictor still not added yet\n",
    "    accs <- list()\n",
    "    models <- list()\n",
    "    for (j in 1:length(names)) {\n",
    "        # create a model string for this combination of predictors\n",
    "        preds_new <- c(selected, names[[j]])\n",
    "        model_string <- paste(\"DEATH_EVENT\", \"~\", paste(preds_new, collapse=\"+\"))\n",
    "\n",
    "        # create a recipe from the model string\n",
    "        heart_recipe <- recipe(as.formula(model_string), \n",
    "                                data = heart_train) %>% \n",
    "                          step_scale(all_predictors()) %>% \n",
    "                          step_center(all_predictors())\n",
    "\n",
    "        # tune the KNN classifier with these predictors, \n",
    "        # and collect the accuracy for the best K\n",
    "        acc <- workflow() %>% \n",
    "              add_recipe(heart_recipe) %>% \n",
    "              add_model(heart_spec) %>% \n",
    "              tune_grid(resamples = heart_vfold, grid = k_vals) %>% \n",
    "              collect_metrics() %>% \n",
    "              filter(.metric == \"accuracy\") %>% \n",
    "              summarize(mx = max(mean))\n",
    "        acc <- acc$mx %>% unlist()\n",
    "\n",
    "        # add this result to the dataframe\n",
    "        accs[[j]] <- acc\n",
    "        models[[j]] <- model_string\n",
    "    }\n",
    "    jstar <- which.max(unlist(accs))\n",
    "    accuracies <- accuracies %>% \n",
    "      add_row(size = i, \n",
    "              model_string = models[[jstar]], \n",
    "              accuracy = accs[[jstar]])\n",
    "    selected <- c(selected, names[[jstar]])\n",
    "    names <- names[-jstar]\n",
    "}\n",
    "\n",
    "accuracies\n",
    "\n",
    "# Working: \n",
    "# A k-nn classifier is built for each set of predictors, \n",
    "# its best k value (from k = 1 to 15) and its appropriate accuracy is calculated with 5-v fold cross validation. \n",
    "# This was achievable primarily due to the “paste” function which helped \n",
    "# form a model specification for each set of predictor by concatinating them seperated with the “+” signs. \n",
    "# Moreover, two for loops were used to achieve this: the first one to account for the increasing predictor set sizes, \n",
    "# and the second one to analyse which predictor to add to each cycle/iteration. An important \n",
    "# factor which made us select this algorithm for our predictor variable selection was computational cost. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table 2: Table containg results from forward propagation in ascending order of number of variables used "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method (forward propagation): This code block ran a predictor variable selection algorithm called \"forward propagation\". This algorithm essentially “propagates” through all the variables, concatenating singular predictor variables after each other for every iteration of the loop. The subset of variables that return the higher values of accuracies is the ones which are appropriate for the classification. There are many other (more productive) algorithms present for this task such as best subset selection, however, most of them take much longer to run while not giving any significant difference in the overall accuracy of the classification. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result (forward propagation): The result of this evaluation showed that the top 2 accuracies are the second and the third rows. The accuracy predicted with ejection_fraction and serum_creatinine was 79.51% and that predicted with ejection_fraction, serum_creatinine, and serum_sodium was 79.94%. However, biologically, ejection fraction and serum creatinine alone are the best predictors of death events due to medical significance (Chicco, 2020). This does not differ from our evaluation as the accuracy of prediction with ejection_fraction and serum_creatinine is the second-highest. Therefore, these two numerical variables were chosen as predictive variables for our classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting only appropriate columns from training set and testing set (ejection_fraction, serum_creatinine, DEATH_EVENT)\n",
    "heart_train <- heart_train %>% \n",
    "    select(ejection_fraction, serum_creatinine, DEATH_EVENT)\n",
    "\n",
    "heart_test<- heart_test%>%\n",
    "    select(ejection_fraction, serum_creatinine, DEATH_EVENT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options(repr.plot.height = 10, repr.plot.width = 12)\n",
    "\n",
    "# Plotting the initial graph for our variables for exploratory data analysis\n",
    "heart_plot<- heart_train%>%\n",
    "    ggplot(aes(x= serum_creatinine, y = ejection_fraction))+\n",
    "    geom_point(aes(color = DEATH_EVENT), size = 3)+\n",
    "    labs(x = \"Serum Creatinine Levels (mg/dL)\", y = \"Ejection Fraction (%)\", \n",
    "         title = \"Serum Creatinine and Ejection Fraction\\n as Predictors for a Death Event\",\n",
    "         color = \"Patient Outcome\")+\n",
    "    theme(text = element_text(size = 20), plot.title = element_text(hjust = .5)) + \n",
    "    scale_color_manual(labels = c(\"Died\", \"Survived\"), \n",
    "                     values = c(\"orange2\", \"steelblue2\"))\n",
    "heart_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure 1: Scatter plot containg our chosen predictor variables with points color based on the outcome for the patient (died/survived)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plotted this for our initial exploration. The plot above shows the distribution of our two predictors in relation to our variable that we are trying to predict, death event (in this graph it is labeled as patient outcome, but that is the same as death event). We can easily notice that the major cluster of death outcomes is in the region where ejection fractions are between 25% - 60% and when serum creatinine is between 0.3 - 2. Hence a clear decision boundary can be formulated which will be done in further analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making our classifier and using the variables found from forward propagation to find the best k value\n",
    "\n",
    "# Creating variable k_vals which stores the candidate k values in form of a tibble\n",
    "k_vals <- tibble(neighbors = seq(from = 1, to = 15, by = 1))\n",
    "\n",
    "# Creating a recipe with recipe function, class = DEATH_EVENT and predictive variables are ejection_fraction\n",
    "# and serum_creatinine. Only heart_train data is used with both the predictive variables scaled. \n",
    "final_recipe <- recipe(DEATH_EVENT ~ ejection_fraction+serum_creatinine, data = heart_train) %>% \n",
    "    step_scale(all_predictors()) %>% \n",
    "    step_center(all_predictors())\n",
    "\n",
    "# Creating a model specification for k-nn classification: argument for \"neighbors\" is set to tune() to \n",
    "# get best k value for the classification\n",
    "# \"rectangular\" input into the weight_func argument was to give each neighbour only 1 voting power\n",
    "kmin_spec <- nearest_neighbor(weight_func = \"rectangular\", neighbors = tune()) %>% \n",
    "    set_engine(\"kknn\") %>% \n",
    "    set_mode(\"classification\")\n",
    "\n",
    "# Creating a 5 v fold cross validation argument with strata = DEATH_EVENT as that is the variable we want to predict\n",
    "final_vfold <- vfold_cv(heart_train, v = 5, strata = DEATH_EVENT)\n",
    "\n",
    "# Putting everything in a workflow, using tune_grid to repeat on all candidate k value with cross validation \n",
    "# and collect_metrics to calculate the accuracy for each run\n",
    "kmin_results <- workflow() %>%\n",
    "    add_recipe(final_recipe) %>% \n",
    "    add_model(kmin_spec) %>% \n",
    "    tune_grid(resamples = final_vfold, grid = k_vals) %>% \n",
    "    collect_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After predictive variables were chosen, the KNN classifier was created with those variables in order to choose the best k value by setting neighbors argument to \"tune()\". We used 5-fold cross validation as with our data size, it is sufficient to help us avoid any \"lucky\" subsets which might influence the accuracy while being computationally efficient. Similarly, the k values were chosen from 1 to 15 to make our code computationally light while giving us a broad enough range for our data set size of 299 data points. The results for calculating accuracy for each run were then calculated to get the best k value: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting only accuracy values from kminresults using filter function\n",
    "final_accuracies <- kmin_results %>% \n",
    "    filter(.metric == \"accuracy\")\n",
    "\n",
    "# Plotting a neighbors vs accuracy mean graph to see which k is the best\n",
    "accuracy_versus_k <- ggplot(final_accuracies, aes(x = neighbors, y = mean))+\n",
    "      geom_point() +\n",
    "      geom_line() +\n",
    "      labs(x = \"Neighbors\", y = \"Accuracy Estimate\", title = \"K value vs Accuracy Estimate\") +\n",
    "      scale_x_continuous(breaks = seq(0, 14, by = 1)) +  # adjusting the x-axis\n",
    "      scale_y_continuous(limits = c(0.4, 1.0)) + # adjusting the y-axis  \n",
    "      theme(text = element_text(size = 20), plot.title = element_text(hjust = .5))\n",
    "accuracy_versus_k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure 2: Line plot showing the Accuracy Estimate of our model in relation to the amount of k neighbors our model was trained with"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then plotted a graph for the mean accuracy which was received after collecting the metrics for every k value. From figure 2 we can see a slight increase in accuracy to k = 7 and then plateau with notable bumps in accuracy around k = 12 and 14."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the k value which offers the maximum accuracy from kmin_results\n",
    "k_min <- kmin_results %>% \n",
    "    filter(.metric == \"accuracy\") %>% \n",
    "    arrange(-mean) %>% \n",
    "    slice(1) %>% \n",
    "    select(neighbors, mean) %>% \n",
    "    rename(accuracy = mean)\n",
    "k_min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table 3: Table showing the neighbor with the highest accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a table of k values in descending order of accuracy shows us that k = 7 gives us the highest accuracy at 78.7%. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retraining with the chosen K value of 7\n",
    "\n",
    "# Creating the final model specification with the best k value set as the argument for neighbors (7)\n",
    "final_spec <- nearest_neighbor(weight_func = \"rectangular\", neighbors = 7) %>% \n",
    "    set_engine(\"kknn\") %>% \n",
    "    set_mode(\"classification\")\n",
    "\n",
    "# Putting the final model specification and recipe into a wokflow and fitting it to heart_train \n",
    "final_fit  <- workflow() %>%\n",
    "    add_recipe(final_recipe) %>% \n",
    "    add_model(final_spec) %>% \n",
    "    fit(data = heart_train)\n",
    "\n",
    "# predicting the classes of heart test \n",
    "heart_prediction <- predict(final_fit, heart_test) %>% \n",
    "    bind_cols(heart_test)\n",
    "\n",
    "# Evaluating the accuracy of prediction made on heart test\n",
    "heart_prediction_accuracy <- heart_prediction %>% \n",
    "    metrics(truth = DEATH_EVENT, estimate = .pred_class) %>% \n",
    "    filter(.metric == \"accuracy\")%>%\n",
    "    pull(.estimate)\n",
    "heart_prediction_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We calculate the test accuracy by retraining a new classifier on heart_train with the best k value (7). The specification was fitted onto the training data and then the predict function was used to predict the classification of DEATH_EVENT for the testing data. These predicted classifications were then compared to the actual classes. We found our model to accurately predict 70.3% of DEATH_EVENTS in the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_prediction_confusion <- heart_prediction %>% \n",
    "    conf_mat(truth = DEATH_EVENT, estimate = .pred_class)\n",
    "heart_prediction_confusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this confusion matrix, we can see that our model misclassified 11 people who survived but were predicted to have died. Despite this being a misclassification that lowers our overall accuracy, if we just look at those \"critical\" misclassifications our model accuracy goes up to (39+13+11)/(39+11+11+13) ≈ 86.5% (refer to discussion section)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we build a majority classifier (one which predicts died always) to see how well our classifier works against the majority. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#added a reference point using a majority classifier if it predicted died everytime it would have 68% accuracy.\n",
    "heart_data_proportions <- heart_train %>%\n",
    "                      group_by(DEATH_EVENT) %>%\n",
    "                      summarize(n = n()) %>%\n",
    "                      mutate(percent = 100*n/nrow(heart_train))\n",
    "\n",
    "heart_data_proportions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The majority classifier seems to be outputting an accuracy of 68%, which is quite close to our actual testing accuracy. This suggests that the classification classifier which we created is not that strong. This will further be discussed in the discussion section. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making a prediction boundry plot for the classifier (code has been adapted from the DSCI 100 course book.) \n",
    "# (Citation is in references section)\n",
    "\n",
    "options(repr.plot.height = 10, repr.plot.width = 12)\n",
    "\n",
    "#scaling all the data before plotting it\n",
    "graph_data <- heart_data\n",
    "\n",
    "# create the grid of serum/ejection vals, and arrange in a data frame\n",
    "ef_grid <- seq(min(graph_data$ejection_fraction), \n",
    "                max(graph_data$ejection_fraction), \n",
    "                length.out = 100)\n",
    "sc_grid <- seq(min(graph_data$serum_creatinine), \n",
    "                max(graph_data$serum_creatinine), \n",
    "                length.out = 100)\n",
    "asgrid <- as_tibble(expand.grid(ejection_fraction = ef_grid, \n",
    "                                serum_creatinine = sc_grid))\n",
    "\n",
    "# use the fit workflow to make predictions at the grid points\n",
    "knnPredGrid <- predict(final_fit, asgrid)\n",
    "\n",
    "# bind the predictions as a new column with the grid points\n",
    "prediction_table <- bind_cols(knnPredGrid, asgrid) %>% \n",
    "  rename(DEATH_EVENT = .pred_class)\n",
    "\n",
    "# plot:\n",
    "# 1. the colored scatter of the original data\n",
    "# 2. the faded colored scatter for the grid points\n",
    "heart_prediction_plot <- ggplot() +\n",
    "  geom_point(data = graph_data, \n",
    "             mapping = aes(x = serum_creatinine, \n",
    "                           y = ejection_fraction, \n",
    "                           color = DEATH_EVENT), \n",
    "             alpha = 0.75, \n",
    "             size = 3) +\n",
    "  geom_point(data = prediction_table, \n",
    "             mapping = aes(x = serum_creatinine, \n",
    "                           y = ejection_fraction, \n",
    "                           color = DEATH_EVENT), \n",
    "             alpha = 0.09, \n",
    "             size = 7) +\n",
    "  labs(color = \"Diagnosis\", x = \"Serum Creatinine Levels (mg/dL)\", y = \"Ejection Fraction (%)\", \n",
    "         title = \"Serum Creatinine and Ejection Fraction\\n as Predictors for a Death Event\") +\n",
    "  scale_color_manual(labels = c(\"Died\", \"Survived\"), \n",
    "                     values = c(\"orange2\", \"steelblue2\")) +\n",
    "  theme(text = element_text(size = 20), plot.title = element_text(hjust = .5))\n",
    "\n",
    "heart_prediction_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plot essentially summarizes the results of our classifier. We have created a decision boundary plot. The actual points are the points from a scatter plot of Ejection fraction (%) versus Serum Creatinine levels (mg/dL) as predictors for a death event. Orange colour indicates people died and blue colour indicates survived. To achieve this, we created the grid of random creatinine/ejection values and made predictions of labels for these random points. When visualizing, each point for this \"fake\" grid was given a high radius value and low transparency so it looks like the graph is actually shaded. Finally, we overlay this plot with a plot of the actual data points from our heart_data dataset to see if the decision boundary aligns with the actual data. Hence our final result is a plot containing (1) the coloured scatter of the original data and (2) the faded coloured scatter for the grid points.\n",
    "\n",
    "As we can clearly see, the decision boundary aligns with the decision boundary which one would have intuitively created and is covering most of the data points correctly, hence the classifier seems to be working properly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Discussion**\n",
    "\n",
    "\n",
    "To create a model from our given data set we found that our best predictors of Death Events were serum creatine levels and ejection fraction.  Serum creatine is a waste product generated by creatine when muscle breakdowns(Chicco, 2020). Serum creatine is normally removed from the bloodstream via the kidneys but high levels of serum creatine may be an indication of renal dysfunction (Cole et al., 2012).  Renal dysfunction is a common comorbidity with acute and chronic heart failure however the complex interactions between the two are still poorly understood in the scientific community (Cole et al., 2012).  The Ejection Fraction is the proportion of blood pumped out through one contraction of the heart and in this data set is given as a percentage (Chicco, 2020).  While these predictors by themselves each have their own impact and connection to heart failure it was unclear what patterns and trends they would have when used together to predict a Death Event. We began by assembling a model using these two variables as predictors. Through the use of cross validation and tune_grid() function we found a range of k values from 1 to 15 and their corresponding accuracies. From Figure 2 we can see a gentle increase with a peak and plateau of accuracy occurring at k = 7.  While we could have chosen a smaller k value to save on computation time and maintain a similar accuracy, we decided to choose the k with the highest accuracy due to our model being responsible for predicting life and death of patients.  After running our model with k = 7 we found that our model had an accuracy of 70.3%. When creating a confusion matrix it showed that our model had predicted 11 deaths as a classification for patients that had actually survived. Even though our model had incorrectly classified these observations we decided to include them as an accurate prediction, since in a hospital setting this patient would still have survived. The more “critical” misclassifications, where our model predicted someone's survival when they died, added up to 11. If we counted only these “critical” classifications as misclassifications it brought our total accuracy up to 86.5%. Our majority classifier which predicted the death of the patient every time had an accuracy of 68%. When we compare our model on a pure classification accuracy base it did not perform well. 70.3% is not acceptable when that means that potentially 30% of the cases will be misclassified which could result in numerous patients deaths. This is also only slightly higher than our majority classification of 68% meaning that our model is barely outperforming a model that takes in zero input data. If we focus on only the critical misclassifications then our classifier would have predicted only 86.5% of deaths. While this is an improvement, this percentage still seems too low given the nature and importance of what this model is predicting. With 13.5% of cases misclassified as survival when the patient died, it would be difficult for doctors and researchers to trust, and thus it would be hard to allocate their resources properly towards those patients at risk. The low accuracy of our model is perhaps not surprising given the vastly different biological roles our two predictors are involved with. Serum creatinine being related to renal dysfunction and ejection fraction being the heart's ability to pump blood do not appear to be related at all on the surface. While there are possible connections between renal dysfunction and heart failure, it seemed unlikely that those two predictors would be able to accurately predict a patient's chance of survival post heart failure. It is also possible that there is a trend, however, we are limited by our smaller dataset. In the future, we may want to replicate our model with a larger data set giving it more data to train and test with. This findings further complicate the potential connections that can be drawn between kidney failure and heart failure. In order to better understand these connections, future studies may want to investigate other common indicators of kidney failure and heart failure and discover if any patterns or trends lie between them as predictors of survival.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**References**\n",
    "\n",
    "\n",
    "Cole, R. T.; Masoumi, A.; Triposkiadis, F.; Giamouzis, G.; Georgiopoulou, V.; Kalogeropoulos, A.; \n",
    "Butler, J. Renal Dysfunction in Heart Failure. Medical Clinics of North America 2012, 96 (5), 955–974.\n",
    "UCI Machine Learning Repository: Heart Failure Clinical Records Data Set. (n.d.). Retrieved \n",
    "March 5, 2022, from https://archive.ics.uci.edu/ml/datasets/Heart+failure+clinical+records \n",
    "\n",
    "Tiffany Timbers, T. C. (2022, March 2). Data science. Chapter 6 Classification II: evaluation and \n",
    "tuning. Retrieved March 5, 2022, from https://datasciencebook.ca/classification2.html\n",
    "\n",
    "Chicco, D., & Jurman, G. (2020, February 3). Machine learning can predict survival of patients with heart failure from serum creatinine and ejection fraction alone. BMC Medical Informatics and Decision Making, 1. https://doi.org/10.1186/s12911-020-1023-5"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
